{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA-Assignment-7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PuJsfxfq3-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ecc10d40-f731-461e-e38a-b1e62bc23863"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation, SeparableConv2D, concatenate, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import AveragePooling2D, Input, Flatten, concatenate, Lambda, Input, Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model,Sequential\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3VU-oB2q-Xd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32  # orig paper trained all networks with batch_size=128\n",
        "epochs = 200\n",
        "data_augmentation = True\n",
        "num_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFhR92u2rB-Z",
        "colab_type": "code",
        "outputId": "4327f6f5-b609-4266-ae30-aad5c6084d0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xMWGgw4rEzz",
        "colab_type": "code",
        "outputId": "67ac8596-45a1-4ab3-8cab-5ea7dc228d85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "print(input_shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c7XTll9rIIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf1SDJswrPoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tYrnoqJNeRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def space_to_depth_x4(x):\n",
        "#     return tf.space_to_depth(x, block_size=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTDrhF4N_xnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def space_to_depth_x2(x):\n",
        "  import tensorflow as tf\n",
        "  return tf.space_to_depth(x, block_size=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU5Jdv3jrTyc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "outputId": "96d02de6-306c-4cdf-fd5b-3c3282c6ac5a"
      },
      "source": [
        "inputs = Input(shape=(32, 32, 3,))\n",
        "\n",
        "# Stage 1\n",
        "x1 = SeparableConv2D(32,(5,5),border_mode='same', name='Layer1')(inputs)\n",
        "x1 = BatchNormalization(name = 'bn_conv1')(x1)\n",
        "x1 = Activation('relu')(x1)\n",
        "\n",
        "# Stage 2\n",
        "x2 = Conv2D(32, (5,5), strides=(1,1),border_mode='same', name='Layer_2', use_bias=False)(x1)\n",
        "X2 = BatchNormalization(name = 'bn_conv2')(x2)\n",
        "x2 = Activation('relu')(x2)\n",
        "\n",
        "# Stage 3\n",
        "x3 = Conv2D(32, (5,5), strides=(1,1), border_mode='same', name='Layer_3', use_bias=False)(x2)\n",
        "X3 = BatchNormalization(name = 'bn_conv3')(x3)\n",
        "x3 = Activation('relu')(x3)\n",
        "\n",
        "#adding skip_connection of layer 3\n",
        "skip_connection1 = x1\n",
        "x3_concat =  concatenate([x3, x1])\n",
        "\n",
        "# Stage 4\n",
        "x4 = SeparableConv2D(32,(5,5), border_mode='same', name='Layer4')(x3_concat)\n",
        "x4 = BatchNormalization(name = 'bn_conv4')(x4)\n",
        "x4 = Activation('relu')(x4)\n",
        "\n",
        "#adding skip_connection after layer 4\n",
        "skip_connection2 = x1\n",
        "x4_concat =  concatenate([x4, skip_connection2])\n",
        "\n",
        "#max-pooling layer\n",
        "mp1 = MaxPooling2D(pool_size=(2,2))(x4_concat)\n",
        "\n",
        "# Stage 5\n",
        "x5 = SeparableConv2D(32,(3,3), border_mode='same', name='Layer5')(mp1)\n",
        "x5 = BatchNormalization(name = 'bn_conv5')(x5)\n",
        "x5 = Activation('relu')(x5)\n",
        "x5_concat = concatenate([x4,x1])\n",
        "skip_con5= Lambda(space_to_depth_x2)(x5_concat)\n",
        "x5_concat = concatenate([skip_con5, x5])\n",
        "\n",
        "#Stage 6\n",
        "x6 = Conv2D(32, (5,5), strides=(1,1), border_mode='same', name='Layer_6', use_bias=False)(x5_concat)\n",
        "X6 = BatchNormalization(name = 'bn_conv6')(x6)\n",
        "x6 = Activation('relu')(x6)\n",
        "\n",
        "x6_temp = concatenate([x4,x3])\n",
        "x6_temp = Lambda(space_to_depth_x2)(x6_temp)\n",
        "x6_concat = concatenate([x6_temp, x5,x6])\n",
        "\n",
        "#Stage 7\n",
        "x7 = SeparableConv2D(32,(3,3), border_mode='same', name='Layer7')(x6_concat)\n",
        "x7 = BatchNormalization(name = 'bn_conv7')(x7)\n",
        "x7 = Activation('relu')(x7)\n",
        "x7_temp = concatenate([x1, x3, x4])\n",
        "x7_temp = Lambda(space_to_depth_x2)(x7_temp)\n",
        "x7_concat = concatenate([x5, x6, x7_temp])\n",
        "\n",
        "#Stage 8\n",
        "x8 = SeparableConv2D(32,(5,5), border_mode='same', name='Layer8')(x7_concat)\n",
        "x8 = BatchNormalization(name = 'bn_conv8')(x8)\n",
        "x8 = Activation('relu')(x8)\n",
        "x8_temp = concatenate([x1, x4])\n",
        "x8_temp = Lambda(space_to_depth_x2)(x8_temp)\n",
        "x8_concat = concatenate([x5, x7, x8_temp])\n",
        "\n",
        "#Max-Pooling - 2\n",
        "mp2 = MaxPooling2D(pool_size=(2,2))(x8_concat)\n",
        "x6_temp = Lambda(space_to_depth_x2)(x6)\n",
        "mp2_concat = concatenate([mp2, x6_temp])\n",
        "\n",
        "# Stage 9\n",
        "x9 = Conv2D(32, (5,5), strides=(1,1), border_mode='same', name='Layer_9', use_bias=False)(mp2_concat)\n",
        "X9 = BatchNormalization(name = 'bn_conv9')(x9)\n",
        "x9 = Activation('relu')(x9)\n",
        "x9_temp = concatenate([x1, x2])\n",
        "x9_temp = Lambda(space_to_depth_x2)(x9_temp)\n",
        "x9_temp = concatenate([x9_temp, x7])\n",
        "x9_temp = Lambda(space_to_depth_x2)(x9_temp)\n",
        "x9_concat = concatenate([x9, x9_temp])\n",
        "\n",
        "#Stage 10\n",
        "x10 = SeparableConv2D(32,(5,5), border_mode='same', name='Layer10')(x9_concat)\n",
        "x10 = BatchNormalization(name = 'bn_conv10')(x10)\n",
        "x10 = Activation('relu')(x10)\n",
        "x10_temp = concatenate([x2, x3])\n",
        "x10_temp = Lambda(space_to_depth_x2)(x10_temp)\n",
        "x10_temp = concatenate([x10_temp, x5])\n",
        "x10_temp = Lambda(space_to_depth_x2)(x10_temp)\n",
        "x10_temp = concatenate([x10_temp, x9])\n",
        "x10_concat = concatenate([x10_temp, x10])\n",
        "\n",
        "#Stage 11\n",
        "x11 = Conv2D(32, (3,3), strides=(1,1), border_mode='same', name='Layer_11', use_bias=False)(x10_concat)\n",
        "X11 = BatchNormalization(name = 'bn_conv9')(x11)\n",
        "x11 = Activation('relu')(x11)\n",
        "x11_temp = concatenate([x1, x3, x4])\n",
        "x11_temp = Lambda(space_to_depth_x2)(x11_temp)\n",
        "x11_temp = concatenate([x11_temp, x5])\n",
        "x11_temp = Lambda(space_to_depth_x2)(x11_temp)\n",
        "x11_temp = concatenate([x11_temp, x10])\n",
        "x11_concat = concatenate([x11_temp, x11])\n",
        "\n",
        "#stage 12\n",
        "x12 = SeparableConv2D(32,(5,5), border_mode='same', name='Layer12')(x11_concat)\n",
        "x12 = BatchNormalization(name = 'bn_conv12')(x12)\n",
        "x12 = Activation('relu')(x12)\n",
        "x12_temp = Lambda(space_to_depth_x2)(x4)\n",
        "x12_temp = concatenate([x12_temp, x7])\n",
        "x12_temp = Lambda(space_to_depth_x2)(x12_temp)\n",
        "x12_temp = concatenate([x12_temp, x10])\n",
        "x12_concat = concatenate([x12_temp, x12])\n",
        "layer_last = Conv2D(10, (3,3), strides=(1,1), name='Layer_last', use_bias=False)(x12_concat)\n",
        "\n",
        "flatten = GlobalAveragePooling2D()(layer_last)\n",
        "#flatten = Flatten()(layer_last)\n",
        "output = Activation('softmax')(flatten)\n",
        "\n",
        "model = Sequential()\n",
        "model = Model(inputs = [inputs], output = [output])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0622 06:08:17.194133 140528821213056 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0622 06:08:17.230226 140528821213056 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(32, (5, 5), name=\"Layer1\", padding=\"same\")`\n",
            "  after removing the cwd from sys.path.\n",
            "W0622 06:08:17.241065 140528821213056 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0622 06:08:17.298291 140528821213056 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0622 06:08:17.299177 140528821213056 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0622 06:08:20.113463 140528821213056 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), strides=(1, 1), name=\"Layer_2\", use_bias=False, padding=\"same\")`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), strides=(1, 1), name=\"Layer_3\", use_bias=False, padding=\"same\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(32, (5, 5), name=\"Layer4\", padding=\"same\")`\n",
            "W0622 06:08:20.471586 140528821213056 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(32, (3, 3), name=\"Layer5\", padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), strides=(1, 1), name=\"Layer_6\", use_bias=False, padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(32, (3, 3), name=\"Layer7\", padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:60: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(32, (5, 5), name=\"Layer8\", padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:73: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), strides=(1, 1), name=\"Layer_9\", use_bias=False, padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:83: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(32, (5, 5), name=\"Layer10\", padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:94: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), strides=(1, 1), name=\"Layer_11\", use_bias=False, padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:105: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(32, (5, 5), name=\"Layer12\", padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:120: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aPUGtQgbOsc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3212
        },
        "outputId": "04ae30a5-d012-4dbe-b667-40b1bc92c497"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Layer1 (SeparableConv2D)        (None, 32, 32, 32)   203         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv1 (BatchNormalization)   (None, 32, 32, 32)   128         Layer1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 32)   0           bn_conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Layer_2 (Conv2D)                (None, 32, 32, 32)   25600       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 32)   0           Layer_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "Layer_3 (Conv2D)                (None, 32, 32, 32)   25600       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 32)   0           Layer_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 64)   0           activation_3[0][0]               \n",
            "                                                                 activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "Layer4 (SeparableConv2D)        (None, 32, 32, 32)   3680        concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv4 (BatchNormalization)   (None, 32, 32, 32)   128         Layer4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 32)   0           bn_conv4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 64)   0           activation_4[0][0]               \n",
            "                                                                 activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 64)   0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Layer5 (SeparableConv2D)        (None, 16, 16, 32)   2656        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv5 (BatchNormalization)   (None, 16, 16, 32)   128         Layer5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 64)   0           activation_4[0][0]               \n",
            "                                                                 activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 16, 16, 32)   0           bn_conv5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 16, 16, 256)  0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 16, 16, 288)  0           lambda_1[0][0]                   \n",
            "                                                                 activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 64)   0           activation_4[0][0]               \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "Layer_6 (Conv2D)                (None, 16, 16, 32)   230400      concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 16, 16, 256)  0           concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 16, 16, 32)   0           Layer_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 16, 16, 320)  0           lambda_2[0][0]                   \n",
            "                                                                 activation_5[0][0]               \n",
            "                                                                 activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "Layer7 (SeparableConv2D)        (None, 16, 16, 32)   13152       concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv7 (BatchNormalization)   (None, 16, 16, 32)   128         Layer7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 64)   0           activation_1[0][0]               \n",
            "                                                                 activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 32)   0           bn_conv7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 16, 16, 256)  0           concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 16, 16, 320)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 320)    0           concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 8, 8, 128)    0           activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 32, 32, 64)   0           activation_1[0][0]               \n",
            "                                                                 activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 8, 8, 448)    0           max_pooling2d_2[0][0]            \n",
            "                                                                 lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 16, 16, 256)  0           concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Layer_9 (Conv2D)                (None, 8, 8, 32)     358400      concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 288)  0           lambda_6[0][0]                   \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 8, 8, 32)     0           Layer_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 8, 8, 1152)   0           concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 32, 32, 64)   0           activation_2[0][0]               \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 8, 8, 1184)   0           activation_9[0][0]               \n",
            "                                                                 lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 16, 16, 256)  0           concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Layer10 (SeparableConv2D)       (None, 8, 8, 32)     67520       concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 288)  0           lambda_8[0][0]                   \n",
            "                                                                 activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv10 (BatchNormalization)  (None, 8, 8, 32)     128         Layer10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 32, 32, 96)   0           activation_1[0][0]               \n",
            "                                                                 activation_3[0][0]               \n",
            "                                                                 activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 8, 8, 1152)   0           concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 8, 8, 32)     0           bn_conv10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 16, 16, 384)  0           concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 8, 8, 1184)   0           lambda_9[0][0]                   \n",
            "                                                                 activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 416)  0           lambda_10[0][0]                  \n",
            "                                                                 activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 8, 8, 1216)   0           concatenate_17[0][0]             \n",
            "                                                                 activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 8, 8, 1664)   0           concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Layer_11 (Conv2D)               (None, 8, 8, 32)     350208      concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 8, 8, 1696)   0           lambda_11[0][0]                  \n",
            "                                                                 activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 8, 8, 32)     0           Layer_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_12 (Lambda)              (None, 16, 16, 128)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 8, 8, 1728)   0           concatenate_21[0][0]             \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 160)  0           lambda_12[0][0]                  \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "Layer12 (SeparableConv2D)       (None, 8, 8, 32)     98528       concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_13 (Lambda)              (None, 8, 8, 640)    0           concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv12 (BatchNormalization)  (None, 8, 8, 32)     128         Layer12[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 8, 8, 672)    0           lambda_13[0][0]                  \n",
            "                                                                 activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 8, 8, 32)     0           bn_conv12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 8, 8, 704)    0           concatenate_24[0][0]             \n",
            "                                                                 activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Layer_last (Conv2D)             (None, 6, 6, 10)     63360       concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 10)           0           Layer_last[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 10)           0           global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 1,240,075\n",
            "Trainable params: 1,239,691\n",
            "Non-trainable params: 384\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqCp3K_Dskwf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3772
        },
        "outputId": "054caf28-d5fc-4f26-d0b9-25c36bb0b39f"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=100, verbose=1, validation_data=(x_test,y_test))\n",
        "model.save('my_model.hdf5')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "50000/50000 [==============================] - 43s 853us/step - loss: 0.1146 - acc: 0.9590 - val_loss: 1.3235 - val_acc: 0.7293\n",
            "Epoch 2/100\n",
            "50000/50000 [==============================] - 41s 813us/step - loss: 0.1003 - acc: 0.9642 - val_loss: 1.7341 - val_acc: 0.6764\n",
            "Epoch 3/100\n",
            "50000/50000 [==============================] - 40s 809us/step - loss: 0.0895 - acc: 0.9688 - val_loss: 1.4110 - val_acc: 0.7159\n",
            "Epoch 4/100\n",
            "50000/50000 [==============================] - 41s 813us/step - loss: 0.0797 - acc: 0.9728 - val_loss: 1.6078 - val_acc: 0.6829\n",
            "Epoch 5/100\n",
            "50000/50000 [==============================] - 41s 813us/step - loss: 0.0843 - acc: 0.9693 - val_loss: 2.1807 - val_acc: 0.6280\n",
            "Epoch 6/100\n",
            "50000/50000 [==============================] - 41s 818us/step - loss: 0.0780 - acc: 0.9719 - val_loss: 1.6433 - val_acc: 0.6939\n",
            "Epoch 7/100\n",
            "50000/50000 [==============================] - 41s 813us/step - loss: 0.0707 - acc: 0.9754 - val_loss: 2.1534 - val_acc: 0.6372\n",
            "Epoch 8/100\n",
            "50000/50000 [==============================] - 41s 814us/step - loss: 0.0755 - acc: 0.9725 - val_loss: 1.4717 - val_acc: 0.7238\n",
            "Epoch 9/100\n",
            "50000/50000 [==============================] - 41s 814us/step - loss: 0.0589 - acc: 0.9788 - val_loss: 1.8834 - val_acc: 0.6788\n",
            "Epoch 10/100\n",
            "50000/50000 [==============================] - 41s 815us/step - loss: 0.0584 - acc: 0.9796 - val_loss: 1.5209 - val_acc: 0.7051\n",
            "Epoch 11/100\n",
            "50000/50000 [==============================] - 50s 1ms/step - loss: 0.0698 - acc: 0.9756 - val_loss: 1.5916 - val_acc: 0.7088\n",
            "Epoch 12/100\n",
            "50000/50000 [==============================] - 75s 1ms/step - loss: 0.0579 - acc: 0.9791 - val_loss: 1.8517 - val_acc: 0.6957\n",
            "Epoch 13/100\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 0.0609 - acc: 0.9791 - val_loss: 1.5483 - val_acc: 0.7275\n",
            "Epoch 14/100\n",
            "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0478 - acc: 0.9838 - val_loss: 1.3825 - val_acc: 0.7397\n",
            "Epoch 15/100\n",
            "50000/50000 [==============================] - 72s 1ms/step - loss: 0.0592 - acc: 0.9794 - val_loss: 2.3902 - val_acc: 0.6600\n",
            "Epoch 16/100\n",
            "50000/50000 [==============================] - 72s 1ms/step - loss: 0.0594 - acc: 0.9789 - val_loss: 1.8227 - val_acc: 0.7088\n",
            "Epoch 17/100\n",
            "50000/50000 [==============================] - 72s 1ms/step - loss: 0.0460 - acc: 0.9840 - val_loss: 1.6406 - val_acc: 0.7190\n",
            "Epoch 18/100\n",
            "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0495 - acc: 0.9834 - val_loss: 1.8915 - val_acc: 0.6967\n",
            "Epoch 19/100\n",
            "50000/50000 [==============================] - 72s 1ms/step - loss: 0.0443 - acc: 0.9844 - val_loss: 1.5979 - val_acc: 0.7319\n",
            "Epoch 20/100\n",
            "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0491 - acc: 0.9832 - val_loss: 2.0479 - val_acc: 0.6857\n",
            "Epoch 21/100\n",
            "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0465 - acc: 0.9838 - val_loss: 2.0549 - val_acc: 0.6793\n",
            "Epoch 22/100\n",
            "50000/50000 [==============================] - 72s 1ms/step - loss: 0.0482 - acc: 0.9835 - val_loss: 1.5976 - val_acc: 0.7382\n",
            "Epoch 23/100\n",
            "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0408 - acc: 0.9860 - val_loss: 2.1828 - val_acc: 0.6733\n",
            "Epoch 24/100\n",
            "50000/50000 [==============================] - 72s 1ms/step - loss: 0.0495 - acc: 0.9826 - val_loss: 2.0219 - val_acc: 0.6870\n",
            "Epoch 25/100\n",
            "50000/50000 [==============================] - 72s 1ms/step - loss: 0.0390 - acc: 0.9867 - val_loss: 1.9054 - val_acc: 0.7030\n",
            "Epoch 26/100\n",
            "50000/50000 [==============================] - 72s 1ms/step - loss: 0.0321 - acc: 0.9888 - val_loss: 2.5880 - val_acc: 0.6513\n",
            "Epoch 27/100\n",
            "50000/50000 [==============================] - 63s 1ms/step - loss: 0.0512 - acc: 0.9825 - val_loss: 1.7346 - val_acc: 0.7289\n",
            "Epoch 28/100\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.0392 - acc: 0.9867 - val_loss: 1.7100 - val_acc: 0.7118\n",
            "Epoch 29/100\n",
            "50000/50000 [==============================] - 40s 807us/step - loss: 0.0329 - acc: 0.9885 - val_loss: 2.4454 - val_acc: 0.6719\n",
            "Epoch 30/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0398 - acc: 0.9867 - val_loss: 1.8709 - val_acc: 0.7271\n",
            "Epoch 31/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0363 - acc: 0.9876 - val_loss: 2.1406 - val_acc: 0.6915\n",
            "Epoch 32/100\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.0455 - acc: 0.9843 - val_loss: 2.1505 - val_acc: 0.6944\n",
            "Epoch 33/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0323 - acc: 0.9887 - val_loss: 1.7163 - val_acc: 0.7317\n",
            "Epoch 34/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0389 - acc: 0.9862 - val_loss: 1.7767 - val_acc: 0.7301\n",
            "Epoch 35/100\n",
            "50000/50000 [==============================] - 40s 807us/step - loss: 0.0323 - acc: 0.9890 - val_loss: 1.8200 - val_acc: 0.7313\n",
            "Epoch 36/100\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.0339 - acc: 0.9883 - val_loss: 1.8467 - val_acc: 0.7216\n",
            "Epoch 37/100\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.0305 - acc: 0.9895 - val_loss: 1.8163 - val_acc: 0.7253\n",
            "Epoch 38/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0366 - acc: 0.9876 - val_loss: 1.8383 - val_acc: 0.7170\n",
            "Epoch 39/100\n",
            "50000/50000 [==============================] - 40s 807us/step - loss: 0.0358 - acc: 0.9877 - val_loss: 1.8598 - val_acc: 0.7007\n",
            "Epoch 40/100\n",
            "50000/50000 [==============================] - 40s 810us/step - loss: 0.0300 - acc: 0.9901 - val_loss: 1.7504 - val_acc: 0.7280\n",
            "Epoch 41/100\n",
            "50000/50000 [==============================] - 40s 810us/step - loss: 0.0312 - acc: 0.9890 - val_loss: 1.8370 - val_acc: 0.7289\n",
            "Epoch 42/100\n",
            "50000/50000 [==============================] - 40s 809us/step - loss: 0.0310 - acc: 0.9890 - val_loss: 1.9869 - val_acc: 0.7110\n",
            "Epoch 43/100\n",
            "50000/50000 [==============================] - 41s 811us/step - loss: 0.0328 - acc: 0.9887 - val_loss: 1.8331 - val_acc: 0.7277\n",
            "Epoch 44/100\n",
            "50000/50000 [==============================] - 40s 809us/step - loss: 0.0300 - acc: 0.9899 - val_loss: 1.9147 - val_acc: 0.7306\n",
            "Epoch 45/100\n",
            "50000/50000 [==============================] - 40s 808us/step - loss: 0.0309 - acc: 0.9890 - val_loss: 2.0880 - val_acc: 0.7095\n",
            "Epoch 46/100\n",
            "50000/50000 [==============================] - 40s 808us/step - loss: 0.0265 - acc: 0.9908 - val_loss: 1.8808 - val_acc: 0.7300\n",
            "Epoch 47/100\n",
            "50000/50000 [==============================] - 40s 807us/step - loss: 0.0271 - acc: 0.9909 - val_loss: 2.2883 - val_acc: 0.6952\n",
            "Epoch 48/100\n",
            "50000/50000 [==============================] - 40s 807us/step - loss: 0.0276 - acc: 0.9907 - val_loss: 2.2193 - val_acc: 0.6954\n",
            "Epoch 49/100\n",
            "50000/50000 [==============================] - 40s 807us/step - loss: 0.0242 - acc: 0.9911 - val_loss: 2.0808 - val_acc: 0.7076\n",
            "Epoch 50/100\n",
            "50000/50000 [==============================] - 40s 807us/step - loss: 0.0255 - acc: 0.9913 - val_loss: 1.9432 - val_acc: 0.7258\n",
            "Epoch 51/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0361 - acc: 0.9877 - val_loss: 2.1441 - val_acc: 0.6930\n",
            "Epoch 52/100\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.0232 - acc: 0.9924 - val_loss: 1.8804 - val_acc: 0.7269\n",
            "Epoch 53/100\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.0274 - acc: 0.9902 - val_loss: 2.0684 - val_acc: 0.7174\n",
            "Epoch 54/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0197 - acc: 0.9931 - val_loss: 1.8770 - val_acc: 0.7267\n",
            "Epoch 55/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0316 - acc: 0.9891 - val_loss: 2.3290 - val_acc: 0.6803\n",
            "Epoch 56/100\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.0256 - acc: 0.9914 - val_loss: 1.9823 - val_acc: 0.7091\n",
            "Epoch 57/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0291 - acc: 0.9904 - val_loss: 1.9286 - val_acc: 0.7292\n",
            "Epoch 58/100\n",
            "50000/50000 [==============================] - 40s 808us/step - loss: 0.0229 - acc: 0.9921 - val_loss: 1.8192 - val_acc: 0.7359\n",
            "Epoch 59/100\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.0276 - acc: 0.9907 - val_loss: 1.8703 - val_acc: 0.7377\n",
            "Epoch 60/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0230 - acc: 0.9924 - val_loss: 1.9825 - val_acc: 0.7290\n",
            "Epoch 61/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0274 - acc: 0.9909 - val_loss: 1.8707 - val_acc: 0.7332\n",
            "Epoch 62/100\n",
            "50000/50000 [==============================] - 40s 804us/step - loss: 0.0185 - acc: 0.9937 - val_loss: 1.9185 - val_acc: 0.7245\n",
            "Epoch 63/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0308 - acc: 0.9891 - val_loss: 1.9299 - val_acc: 0.7326\n",
            "Epoch 64/100\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.0220 - acc: 0.9926 - val_loss: 1.9523 - val_acc: 0.7296\n",
            "Epoch 65/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0192 - acc: 0.9932 - val_loss: 2.3518 - val_acc: 0.6917\n",
            "Epoch 66/100\n",
            "50000/50000 [==============================] - 40s 809us/step - loss: 0.0222 - acc: 0.9924 - val_loss: 1.8928 - val_acc: 0.7290\n",
            "Epoch 67/100\n",
            "50000/50000 [==============================] - 40s 808us/step - loss: 0.0260 - acc: 0.9912 - val_loss: 2.1217 - val_acc: 0.7128\n",
            "Epoch 68/100\n",
            "50000/50000 [==============================] - 40s 807us/step - loss: 0.0164 - acc: 0.9943 - val_loss: 2.0051 - val_acc: 0.7248\n",
            "Epoch 69/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0229 - acc: 0.9923 - val_loss: 1.9767 - val_acc: 0.7367\n",
            "Epoch 70/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0250 - acc: 0.9914 - val_loss: 2.0453 - val_acc: 0.7220\n",
            "Epoch 71/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0188 - acc: 0.9938 - val_loss: 1.8808 - val_acc: 0.7425\n",
            "Epoch 72/100\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.0225 - acc: 0.9925 - val_loss: 1.9880 - val_acc: 0.7290\n",
            "Epoch 73/100\n",
            "50000/50000 [==============================] - 40s 807us/step - loss: 0.0188 - acc: 0.9936 - val_loss: 1.9800 - val_acc: 0.7322\n",
            "Epoch 74/100\n",
            "50000/50000 [==============================] - 40s 807us/step - loss: 0.0210 - acc: 0.9926 - val_loss: 2.2110 - val_acc: 0.7009\n",
            "Epoch 75/100\n",
            "50000/50000 [==============================] - 40s 807us/step - loss: 0.0199 - acc: 0.9936 - val_loss: 2.9819 - val_acc: 0.6278\n",
            "Epoch 76/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0246 - acc: 0.9918 - val_loss: 1.9325 - val_acc: 0.7381\n",
            "Epoch 77/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0166 - acc: 0.9944 - val_loss: 1.9149 - val_acc: 0.7413\n",
            "Epoch 78/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0213 - acc: 0.9926 - val_loss: 2.2808 - val_acc: 0.7141\n",
            "Epoch 79/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0228 - acc: 0.9925 - val_loss: 1.9782 - val_acc: 0.7327\n",
            "Epoch 80/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0202 - acc: 0.9935 - val_loss: 2.0769 - val_acc: 0.7171\n",
            "Epoch 81/100\n",
            "50000/50000 [==============================] - 40s 807us/step - loss: 0.0150 - acc: 0.9947 - val_loss: 2.0938 - val_acc: 0.7234\n",
            "Epoch 82/100\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.0251 - acc: 0.9919 - val_loss: 2.0308 - val_acc: 0.7185\n",
            "Epoch 83/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0186 - acc: 0.9938 - val_loss: 1.9107 - val_acc: 0.7420\n",
            "Epoch 84/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0195 - acc: 0.9937 - val_loss: 2.0481 - val_acc: 0.7325\n",
            "Epoch 85/100\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.0177 - acc: 0.9940 - val_loss: 1.9482 - val_acc: 0.7377\n",
            "Epoch 86/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0194 - acc: 0.9931 - val_loss: 2.7251 - val_acc: 0.6516\n",
            "Epoch 87/100\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.0229 - acc: 0.9923 - val_loss: 2.6843 - val_acc: 0.6708\n",
            "Epoch 88/100\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.0179 - acc: 0.9943 - val_loss: 2.0780 - val_acc: 0.7246\n",
            "Epoch 89/100\n",
            "50000/50000 [==============================] - 40s 807us/step - loss: 0.0169 - acc: 0.9939 - val_loss: 2.0430 - val_acc: 0.7294\n",
            "Epoch 90/100\n",
            "50000/50000 [==============================] - 40s 807us/step - loss: 0.0175 - acc: 0.9937 - val_loss: 2.2118 - val_acc: 0.7176\n",
            "Epoch 91/100\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.0246 - acc: 0.9914 - val_loss: 2.1098 - val_acc: 0.7231\n",
            "Epoch 92/100\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.0127 - acc: 0.9954 - val_loss: 2.2003 - val_acc: 0.7175\n",
            "Epoch 93/100\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.0123 - acc: 0.9959 - val_loss: 2.1353 - val_acc: 0.7268\n",
            "Epoch 94/100\n",
            "50000/50000 [==============================] - 40s 804us/step - loss: 0.0236 - acc: 0.9918 - val_loss: 2.1783 - val_acc: 0.7179\n",
            "Epoch 95/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0162 - acc: 0.9947 - val_loss: 2.1084 - val_acc: 0.7238\n",
            "Epoch 96/100\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.0179 - acc: 0.9940 - val_loss: 2.0266 - val_acc: 0.7303\n",
            "Epoch 97/100\n",
            "50000/50000 [==============================] - 40s 807us/step - loss: 0.0188 - acc: 0.9934 - val_loss: 2.1413 - val_acc: 0.7232\n",
            "Epoch 98/100\n",
            "50000/50000 [==============================] - 40s 804us/step - loss: 0.0150 - acc: 0.9951 - val_loss: 2.2167 - val_acc: 0.7281\n",
            "Epoch 99/100\n",
            "50000/50000 [==============================] - 40s 804us/step - loss: 0.0150 - acc: 0.9948 - val_loss: 2.0649 - val_acc: 0.7078\n",
            "Epoch 100/100\n",
            "50000/50000 [==============================] - 40s 804us/step - loss: 0.0156 - acc: 0.9946 - val_loss: 2.2900 - val_acc: 0.7137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noWYPWQ-tAvJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "121a0536-3ee2-4109-b196-0c0fe52e76c9"
      },
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 3s 305us/step\n",
            "Test loss: 2.290043265914917\n",
            "Test accuracy: 0.7137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT7Y-WXPJF_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD6j_h6DJgHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}