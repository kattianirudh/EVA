## **Architectural Basics.**

This contains the order of the things in decreasing importance.

1) Kernels and how do we decide the number of kernels?

2) Receptive Field

3) 3x3 Convolutions 

4)  1x1 Convolutions

5) MaxPooling

6) Position of MaxPooling

7) How do we know our network is not going well, comparatively, very early

8) Number of Epochs and when to increase them

9) DropOut

10) When do we introduce DropOut, or when do we know we have some overfitting

11)  Batch Size, and effects of batch size

12) The distance of Batch Normalization from Prediction, 

13) LR schedule and concept behind it

14) Batch Normalization

15)  SoftMax

16) Learning Rate, 

17)  Image Normalization, 

18)  Concept of Transition Layers, 

19)  Adam vs SGD

20)  When to add validation checks

21) Position of Transition Layer

22) When do we stop convolutions and go ahead with a larger kernel or some other alternative (which we have not yet covered)

23) How many layers

24)  LearningRateScheduler

